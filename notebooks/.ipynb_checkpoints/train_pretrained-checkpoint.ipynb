{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow  import keras\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "\n",
    "import librosa \n",
    "from util import WavDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "from config import INTERMEDIATE, MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "hdf5_dataset = h5py.File(INTERMEDIATE / 'train.hdf5', 'r')\n",
    "random_chunks = np.array(hdf5_dataset)\n",
    "np.random.shuffle(random_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "458.73566 0\t\t594.2076 0\t\t746.2246 0\t\t531.56323 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 11:10:02.884485: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t\t657.0123 0\t\t626.8448 0\t\t664.3427 4\t\t531.1446 0\t\t777.21875 0\t\t763.9229 0\t\t493.90265 0\t\t692.4295 0\t\t566.7892 0\t\t554.7632 0\t\t578.3624 0\t\t545.44275 0\t\t787.87085 0\t\t471.79166 0\t\t701.3546 0\t\t568.9809 0\t\t637.3032 3\t\t641.09607 0\t\t561.34283 0\t\t594.9181 0\t\t801.41156 0\t\t740.50226 0\t\t781.42566 0\t\t628.4475 0\t\t667.9726 0\t\t725.094 0\t\t635.2217 0\t\t614.97064 0\t\t"
     ]
    }
   ],
   "source": [
    "X_shape = (10, 1024)\n",
    "Y_shape = (10, 4)\n",
    "\n",
    "def chunk_generator():\n",
    "    for chunk in random_chunks:\n",
    "        chunk_group = hdf5_dataset[chunk]\n",
    "        yield (\n",
    "            np.array(chunk_group['X']), \n",
    "            np.array(chunk_group['Y']).T) \n",
    "\n",
    "raw_dataset = tf.data.Dataset.from_generator(\n",
    "    chunk_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=X_shape, dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=Y_shape, dtype=tf.bool)))\n",
    "\n",
    "# test\n",
    "for s in raw_dataset.take(32):\n",
    "    X, Y = s\n",
    "    print(np.array(X).sum(), np.array(Y).sum(), end=\"\\t\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10, 256)           262400    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10, 128)           32896     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10, 64)            8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10, 4)             260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 303,812\n",
      "Trainable params: 303,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        Input(shape=(10, 1024)),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(4, activation='sigmoid')\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',  \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/4185 (per epoch)\n",
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "# 133921 chunks\n",
    "# 4185 batches\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pandas as pd\n",
    "\n",
    "# split\n",
    "n_chunks =  133921 \n",
    "cut = int(n_chunks * 0.8)\n",
    "train_dataset = raw_dataset.take(cut).batch(32).prefetch(50)\n",
    "validation_dataset = raw_dataset.skip(cut).batch(32).prefetch(50)\n",
    "\n",
    "# callbacks\n",
    "log_dir = Path(\"logs\") / \"fit\"\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir)\n",
    "checkpoint_path = MODEL_DIR / \"training_1\" / \"crnn.ckpt\"         \n",
    "Path(checkpoint_path).parent.mkdir(exist_ok=True)                \n",
    "cp_callback = keras.callbacks.ModelCheckpoint(                   \n",
    "    filepath=checkpoint_path, save_weights_only=True, verbose=1  \n",
    ")                                                                \n",
    "\n",
    "# fit\n",
    "print(\"/4185 (per epoch)\")\n",
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    validation_data=validation_dataset,\n",
    "    epochs=20, \n",
    "    verbose=2,\n",
    "    callbacks=[tensorboard_callback, cp_callback])\n",
    "\n",
    "df = pd.DataFrame(history.history)\n",
    "df.to_csv(\"logs/history.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
