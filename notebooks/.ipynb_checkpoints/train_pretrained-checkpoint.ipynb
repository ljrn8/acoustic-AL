{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow  import keras\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "\n",
    "import librosa \n",
    "from util import WavDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr = 16_000\n",
    "\n",
    "def compute_frame_labels(label_tensor, frame_length=int(sr*0.96), step_size=int(sr*0.48), threshold=0.15):\n",
    "    n_labels, total_samples = label_tensor.shape\n",
    "    n_frames = total_samples // step_size \n",
    "    \n",
    "    frame_labels = np.zeros((n_labels, n_frames), dtype=int)\n",
    "    \n",
    "    for i in range(n_frames):\n",
    "        start = i * step_size\n",
    "        end = start + frame_length\n",
    "        frame = label_tensor[:, start:end]\n",
    "        \n",
    "        # is there >15% annotations in the frame\n",
    "        frame_label = (np.mean(frame, axis=1) >= threshold).astype(int)\n",
    "        frame_labels[:, i] = frame_label\n",
    "    \n",
    "    return frame_labels\n",
    "\n",
    "Y = np.zeros((4, 16_000 * 5))\n",
    "Y[0, 32_000:] = 1\n",
    "Y[2, :32_000] = 1\n",
    "Y[3, -8_000:] = 1\n",
    "compute_frame_labels(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:14:31 INFO Using /tmp/tfhub_modules to cache modules.\n",
      "2024-09-08 21:14:31.781994: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "yamnet_url = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_layer = hub.KerasLayer(yamnet_url, input_shape=(None,), dtype=tf.float32, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133921\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from config import *\n",
    "from pathlib import Path\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def get_chunk_data(dataset, chunk):\n",
    "    rec, s, e = chunk\n",
    "    Y = dataset[rec]['Y']  \n",
    "    samples = dataset[rec]['X']\n",
    "\n",
    "    # error is here\n",
    "    s_slice = np.array(samples[s:e])\n",
    "    y_slice = np.array(Y[:, s:e])\n",
    "    \n",
    "    if y_slice.shape != (4, 80000):\n",
    "        raise ValueError(y_slice.shape, s_slice.shape, rec, s, e)\n",
    "    \n",
    "    y_frames = compute_frame_labels(y_slice)\n",
    "    return s_slice.T, y_frames.T\n",
    "\n",
    "def chunk_generator():\n",
    "    chunk_len = 5\n",
    "    overlap = 1\n",
    "    sr = 16_000\n",
    "    hdf5_file = INTERMEDIATE / 'train.hdf5'\n",
    "    chunk_len *= sr\n",
    "    overlap *= sr\n",
    "    \n",
    "    with h5py.File(hdf5_file, 'r') as hdf5_dataset:\n",
    "\n",
    "        # get chunk info \n",
    "        chunks = []\n",
    "        for rec in list(hdf5_dataset):\n",
    "            Y = hdf5_dataset[rec]['Y']     \n",
    "            n_samples = Y.shape[1]\n",
    "            chunks += [\n",
    "                (rec, start, start + chunk_len) \n",
    "                for start in range(0, n_samples - chunk_len, chunk_len - overlap)\n",
    "            ]\n",
    "\n",
    "        # shuffle all chunks\n",
    "        np.random.shuffle(chunks)\n",
    "    \n",
    "        # generate chunks\n",
    "        for random_chunk in chunks:\n",
    "            rec, s, e = random_chunk\n",
    "            yield get_chunk_data(hdf5_dataset, random_chunk)\n",
    "            \n",
    "\n",
    "raw_dataset = tf.data.Dataset.from_generator(\n",
    "    chunk_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(10, 4), dtype=tf.bool)))\n",
    "\n",
    "def get_embeddings(sample_chunk, Y_chunk): # ! most computation is spend here\n",
    "    _, emb, _ = yamnet_layer(sample_chunk)\n",
    "    return emb, Y_chunk \n",
    "\n",
    "full_dataset = raw_dataset.map(lambda x, y: get_embeddings(x, y))\n",
    "\n",
    "for s in full_dataset.take(32):\n",
    "    X, Y = s\n",
    "    if (X.shape, Y.shape) != ((10, 1024), (10, 4)):\n",
    "        print(\"!! -> \", X.shape, Y.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 10, 256)           262400    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10, 128)           32896     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10, 64)            8256      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10, 4)             260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 303,812\n",
      "Trainable params: 303,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# inp = keras.Input(shape=(16_000 * 5))\n",
    "# _, embeddings, _ = YAMnet(inp)\n",
    "# output = keras.layers.Dense(4, activation='sigmoid')(embeddings)\n",
    "# model = keras.Sequential(inputs=inp, outputs=output)\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     yamnet_base,\n",
    "#     # tf.keras.layers.GlobalAveragePooling1D(),  # Reduce the dimensionality\n",
    "#     tf.keras.layers.Dense(4, activation='softmax')  # Output layer for classification\n",
    "# ])\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "# inputs = layers.Input(shape=(9, 1024), dtype=tf.float32)\n",
    "# x = layers.Dense(256, activation='relu')(inputs)\n",
    "# x = layers.Dense(128, activation='relu')(x)\n",
    "# x = layers.Dense(64, activation='relu')(x)\n",
    "# outputs = layers.Dense(4, activation='sigmoid')(x)\n",
    "# model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        Input(shape=(10, 1024)),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(4, activation='sigmoid')\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',  \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133921\n",
      "     15/Unknown - 5s 5s/step - loss: 8.2241e-05 - accuracy: 0.0000e+ - 10s 5s/step - loss: 6.1324e-05 - accuracy: 0.0000e+0 - 15s 5s/step - loss: 5.2027e-05 - accuracy: 0.0000e+0 - 19s 5s/step - loss: 0.0071 - accuracy: 0.0000e+00    - 24s 5s/step - loss: 0.0057 - accuracy: 0.0000e+0 - 28s 5s/step - loss: 0.0048 - accuracy: 0.0000e+0 - 33s 5s/step - loss: 0.0041 - accuracy: 0.0000e+0 - 38s 5s/step - loss: 0.0036 - accuracy: 0.0000e+0 - 42s 5s/step - loss: 0.0090 - accuracy: 3.4722e-0 - 47s 5s/step - loss: 0.0081 - accuracy: 0.0050    - 53s 5s/step - loss: 0.0074 - accuracy: 0.008 - 58s 5s/step - loss: 0.0109 - accuracy: 0.010 - 63s 5s/step - loss: 0.0129 - accuracy: 0.010 - 68s 5s/step - loss: 0.0120 - accuracy: 0.010 - 74s 5s/step - loss: 0.0112 - accuracy: 0.0094"
     ]
    }
   ],
   "source": [
    "# 133921 chunks\n",
    "# 4185 batches\n",
    "\n",
    "batched = full_dataset.batch(32)\n",
    "history = model.fit(batched, epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
