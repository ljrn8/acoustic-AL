{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 11:36:29.272348: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-02 11:36:29.272389: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-02 11:36:29.418753: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-02 11:36:29.746947: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-02 11:36:31.500732: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_ranking as tfr\n",
    "\n",
    "from os import path\n",
    "from config import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_frames(hdf5_dataset):\n",
    "    recordings = np.array(hdf5_dataset)\n",
    "    rec =  recordings[0]\n",
    "    embeddings_frames = []\n",
    "    for rec in recordings:\n",
    "        embeddings_frames.extend([\n",
    "            (rec, i) for i in range(hdf5_dataset[rec]['X'].shape[0])\n",
    "        ])\n",
    "        \n",
    "    return embeddings_frames\n",
    "    \n",
    "    \n",
    "hdf5_dataset = h5py.File(INTERMEDIATE / 'embeddings_20p.hdf5', 'r')\n",
    "# embeddings_frames = get_frames(hdf5_dataset)\n",
    "len(hdf5_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelled, unlabelled and test set init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "np.random.shuffle(embeddings_frames) \n",
    "\n",
    "X = np.array([hdf5_dataset[rec]['X'][frame, :] for (rec, frame) in embeddings_frames])\n",
    "Y = np.array([hdf5_dataset[rec]['Y_strict'][:, frame] for (rec, frame) in embeddings_frames])\n",
    "n_frames = len(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1024,), (4,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify\n",
    "X[0].shape, Y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282931 unlabelled \n",
      " 70733 labelled \n",
      " 25.00% initial labelling budget\n",
      "\n",
      "test XY lens: (88416, 88416)\n",
      "unlabelled XY lens: (282931, 282931)\n",
      "labelled XY lens: (70733, 70733)\n"
     ]
    }
   ],
   "source": [
    "n_frames = len(X) \n",
    "test_cut =  int(n_frames * 0.8)\n",
    "label_cut = int(test_cut * 0.8)\n",
    "\n",
    "# test data\n",
    "train_X, train_Y = X[:test_cut],  Y[:test_cut]\n",
    "test_X, test_Y = X[test_cut:],  Y[test_cut:]\n",
    "\n",
    "# labelled and unlabelled data\n",
    "unlabelled_X, unlabelled_Y = train_X[:label_cut], train_Y[:label_cut]\n",
    "labelled_X, labelled_Y = train_X[label_cut:], train_Y[label_cut:]\n",
    "\n",
    "u, t = len(unlabelled_X), len(labelled_Y),  \n",
    "print(f'{u} unlabelled \\n {t} labelled \\n {(t/u)*100:.2f}% initial labelling budget')\n",
    "print(f'\\ntest XY lens: {len(test_X), len(test_Y)}')\n",
    "print(f'unlabelled XY lens: {len(unlabelled_X), len(unlabelled_Y)}')\n",
    "print(f'labelled XY lens: {len(labelled_X), len(labelled_Y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model & Training Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "from functools import partial\n",
    "from keras import metrics\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from util import DEFAULT_TOKENS\n",
    "\n",
    "def create_model() -> keras.Model:\n",
    "    default_dense =  partial(Dense, activation='relu')\n",
    "                            #  kernel_initializer=keras.initializers.LecunNormal(seed=0)) \n",
    "    \n",
    "    return keras.Sequential([\n",
    "            Input(shape=(1024,)),\n",
    "            default_dense(512), \n",
    "            default_dense(256), \n",
    "            default_dense(64), \n",
    "            Dense(4, activation='sigmoid',)\n",
    "    ])\n",
    "\n",
    "def compile(model):\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',  \n",
    "        metrics=[\n",
    "            metrics.Recall(thresholds=0.5),\n",
    "            metrics.Precision(thresholds=0.5),\n",
    "            metrics.AUC(curve='pr', name='auc_pr'),\n",
    "            tfr.keras.metrics.get(key=\"map\", name=\"metric/map\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def oversample_minority_classes(X, Y):\n",
    "    num_classes = Y.shape[1]\n",
    "    class_counts = np.sum(Y, axis=0)\n",
    "    max_count = np.max(class_counts)\n",
    "    new_X = X\n",
    "    new_Y = Y\n",
    "    for class_index in range(num_classes): # 0 1 2 3\n",
    "        class_indices = np.where(Y[:, class_index] == 1)[0] # locations of nr\n",
    "        num_samples_needed = max_count - len(class_indices)\n",
    "        if num_samples_needed > 0:\n",
    "            sampled_indices = np.random.choice(class_indices, num_samples_needed, replace=True)\n",
    "            sampled_X = X[sampled_indices]\n",
    "            sampled_Y = Y[sampled_indices]\n",
    "            \n",
    "            new_X = np.vstack((new_X, sampled_X))\n",
    "            new_Y = np.vstack((new_Y, sampled_Y))\n",
    "            \n",
    "    return new_X, new_Y\n",
    "\n",
    "\n",
    "def undersample(X, Y, reduce_to=0.5): \n",
    "    total_instances = len(X)\n",
    "    assert len(Y) == total_instances\n",
    "    annotated_mask = np.any(Y, axis=1)\n",
    "    annotated_X = X[annotated_mask]\n",
    "    annotated_Y = Y[annotated_mask]\n",
    "    unannotated_mask = ~annotated_mask\n",
    "    unannotated_X = X[unannotated_mask]\n",
    "    unannotated_Y = Y[unannotated_mask]\n",
    "    \n",
    "    n_unannotated_to_sample = int(reduce_to * len(unannotated_X))\n",
    "    if len(unannotated_X) > 0: \n",
    "        sampled_indexes = np.random.choice(len(unannotated_X), size=n_unannotated_to_sample, replace=False)\n",
    "        sampled_X = unannotated_X[sampled_indexes]\n",
    "        sampled_Y = unannotated_Y[sampled_indexes]\n",
    "        new_X = np.vstack((annotated_X, sampled_X))\n",
    "        new_Y = np.vstack((annotated_Y, sampled_Y))\n",
    "    else:\n",
    "        new_X = annotated_X\n",
    "        new_Y = annotated_Y\n",
    "    \n",
    "    return new_X, new_Y\n",
    "\n",
    "\n",
    "def train(model, X, Y, model_dir, reduce_empty_class, stopping_patience=5, \n",
    "          stopping_moniter='loss', epochs=10, **kwargs):\n",
    "    model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # tensorboard\n",
    "    log_dir = model_dir / \"logs\" / \"fit\"\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "    # checkpoints\n",
    "    checkpoint_path = model_dir / \"training\"\n",
    "    Path(checkpoint_path).mkdir(exist_ok=True)  \n",
    "    cp_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_path / 'checkpoint.weights.h5', \n",
    "        save_weights_only=True,\n",
    "        verbose=1, \n",
    "    )\n",
    "\n",
    "    # early stopping\n",
    "    es_checkpoint = EarlyStopping(\n",
    "        monitor=stopping_moniter,\n",
    "        patience=stopping_patience,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "    \n",
    "    histories = []\n",
    "    for epoch in range(epochs): \n",
    "        print(f\"--- Epoch {epoch + 1} ---\")\n",
    "        \n",
    "        # reduce the amount of unlabelled instances by 80%\n",
    "        print(f'previous XY shapes: ', X.shape, Y.shape)\n",
    "        resampled_X, resampled_Y = undersample(X, Y, reduce_to=reduce_empty_class)\n",
    "        resampled_X, resampled_Y = oversample_minority_classes(resampled_X, resampled_Y)\n",
    "        print(f'resampled XY shapes: ', resampled_X.shape, resampled_Y.shape)\n",
    "        for i in range(4):\n",
    "            token_order = list(DEFAULT_TOKENS.keys())\n",
    "            print(f'total of {token_order[i]}) = {resampled_Y[:, i].sum()}')\n",
    "        \n",
    "        history = model.fit(x=resampled_X, y=resampled_Y, \n",
    "                  **kwargs, \n",
    "                  epochs=1, \n",
    "                  verbose=2,\n",
    "                    callbacks=[tensorboard_callback, cp_callback, es_checkpoint])\n",
    "        \n",
    "        histories.append(history)\n",
    "        \n",
    "    df = pd.DataFrame(history.history)\n",
    "    df.to_csv(model_dir / \"history.csv\")\n",
    "    model.save(model_dir / 'model.keras')\n",
    "    return histories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test that it works\n",
    "\n",
    "model = create_model()\n",
    "compile(model)\n",
    "\n",
    "history = train( \n",
    "    model, train_X, train_Y, # essentially training data \n",
    "    epochs=5,\n",
    "    validation_data=(test_X, test_Y),\n",
    "    batch_size=32,\n",
    "    model_dir=MODEL_DIR / 'testing', \n",
    "    reduce_empty_class=0.9,\n",
    "    stopping_moniter='val_loss'\n",
    ")\n",
    "\n",
    "pred_Y = model(test_X)\n",
    "\n",
    "from util import DEFAULT_TOKENS\n",
    "token_order = list(DEFAULT_TOKENS.keys())\n",
    "T = 0.5\n",
    "\n",
    "# look at all precisions and recalls\n",
    "for i in range(4):\n",
    "    print(token_order[i])\n",
    "    m = keras.metrics.Precision(thresholds=T)\n",
    "    m.update_state(test_Y[:, i], pred_Y[:, i])\n",
    "    print('Precision = ', m.result().numpy())\n",
    "\n",
    "    m = keras.metrics.Recall(thresholds=T)\n",
    "    m.update_state(test_Y[:, i], pred_Y[:, i])\n",
    "    print('Recall = ', m.result().numpy())\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 5\n",
    "\n",
    "print(\"RECALL\")\n",
    "for i in (0, 3):\n",
    "    print(f'\\n------ {token_order[i]} -------')\n",
    "    y_true = np.array(test_Y)\n",
    "    pred_Y = np.array(pred_Y)\n",
    "    y_pred_rounded = np.round(pred_Y, 3)\n",
    "\n",
    "    rows = np.where(test_Y[:, i])[0]\n",
    "\n",
    "    for r in rows[:10]:\n",
    "        print(f\"\\nat r={r}\")\n",
    "        print(y_true[r - w: r + w, i].T)\n",
    "        print( y_pred_rounded[r - w: r + w, i].T)\n",
    "        \n",
    "    \n",
    "print(\"\\nPREC\")\n",
    "for i in (0, 3):\n",
    "    print(f'\\n------ {token_order[i]} -------')\n",
    "    y_true = np.array(test_Y)\n",
    "    pred_Y = np.array(pred_Y)\n",
    "    y_pred_rounded = np.round(pred_Y, 3)\n",
    "\n",
    "    rows = np.where(pred_Y[:, i] > 0.5)[0]\n",
    "\n",
    "    for r in rows[:5]:\n",
    "        print(f\"\\nat r={r}\")\n",
    "        print(y_true[r - w: r + w, i].T)\n",
    "        print(y_pred_rounded[r - w: r + w, i].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AL utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def AL_simulation(unlabelled: tuple, intial_labelled: tuple, test: tuple,\n",
    "                    model, name, sampling_query, pool_size, evaluate_metrics,\n",
    "                    num_iterations=10, epochs=10):\n",
    "    \n",
    "    \n",
    "    \n",
    "    unlabelled_X, unlabelled_Y = unlabelled\n",
    "    labelled_X, labelled_Y = intial_labelled\n",
    "    test_X, test_Y = test\n",
    "    \n",
    "    histories = []\n",
    "    iteration_metrics = {}\n",
    "    dir = MODEL_DIR / name\n",
    "    Path(dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        print(f\" --- Iteration {i + 1} --- \")\n",
    "        \n",
    "        # model is recompiled per epoch\n",
    "        model = create_model()\n",
    "        compile(model)\n",
    "        \n",
    "        history = train( \n",
    "            model, labelled_X, labelled_Y,\n",
    "            epochs=epochs,\n",
    "            batch_size=32,\n",
    "            model_dir=MODEL_DIR / dir, \n",
    "        )\n",
    "        histories.append(history)\n",
    "        \n",
    "        # get metrics from test set\n",
    "        y_pred = model.predict(test_X)\n",
    "        y_true = test_Y\n",
    "        budget = (len(labelled_Y) / len(train_X))\n",
    "        iteration_metrics[f'labelling_budget={budget:.2f}'] = evaluate_metrics(y_true, y_pred)\n",
    "        \n",
    "        # active learning query on unlabelled set\n",
    "        y_pred = model.predict(unlabelled_X)\n",
    "        indexes_pool = sampling_query(y_pred, unlabelled_X, pool_size)\n",
    "        X_new = [unlabelled_X[i] for i in indexes_pool]\n",
    "        Y_new = [unlabelled_Y[i] for i in indexes_pool]\n",
    "        \n",
    "        # # add the newly 'annotated' samples to the labelled set        \n",
    "        labelled_X = np.vstack((labelled_X, X_new))\n",
    "        labelled_Y = np.vstack((labelled_Y, Y_new))\n",
    "        \n",
    "        # remove the new samples from the unlabelled set\n",
    "        mask = np.ones(len(unlabelled_X), dtype=bool)\n",
    "        mask[indexes_pool] = False\n",
    "        unlabelled_X = unlabelled_X[mask]\n",
    "        unlabelled_Y = unlabelled_Y[mask]\n",
    "        \n",
    "        print(\"current iteration metrics: \", iteration_metrics)\n",
    "        pd.DataFrame(history.history).to_csv(dir / 'hist.csv')\n",
    "        np.save(dir / 'metrics.npy', iteration_metrics)\n",
    "        gc.collect()\n",
    "\n",
    "    return model, iteration_metrics, histories "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Xgboost test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [07:11:27] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\", \"verbose\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [07:12:08] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\", \"verbose\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [07:12:57] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\", \"verbose\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [07:13:48] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\", \"verbose\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.3081\n",
      "Accuracy: 0.9860\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "train_X, train_Y = unlabelled_X, unlabelled_Y\n",
    "resampled_X, resampled_Y = undersample(X, Y, reduce_to=0.5) # NOTE try reducing this?\n",
    "resampled_X, resampled_Y = oversample_minority_classes(resampled_X, resampled_Y)\n",
    "\n",
    "# Train a separate XGBoost model for each class\n",
    "models = []\n",
    "for i in range(Y.shape[1]):\n",
    "    print(i)\n",
    "    \n",
    "    scale_pos_weight = (resampled_Y.shape[0] / resampled_Y[:, i].sum())\n",
    "    model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', verbose=1,\n",
    "                              scale_pos_weight=scale_pos_weight)\n",
    "    \n",
    "    model.fit(train_X, train_Y[:, i])\n",
    "    models.append(model)\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "pred_Y = np.array([model.predict(test_X) for model in models]).T\n",
    "\n",
    "# Evaluate the model\n",
    "f1 = f1_score(test_Y, pred_Y, average='macro')\n",
    "accuracy = accuracy_score(test_Y, pred_Y)\n",
    "\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88416, 4)\n",
      "fast_trill_6khz\n",
      "0.19254658385093168 0.29523809523809524\n",
      "\n",
      "nr_syllable_3khz\n",
      "0.6472222222222223 0.6246648793565683\n",
      "\n",
      "triangle_3khz\n",
      "0.25862068965517243 0.11811023622047244\n",
      "\n",
      "upsweep_500hz\n",
      "0.2641509433962264 0.16279069767441862\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "pred_Y = np.array([model.predict(test_X) for model in models]).T\n",
    "print(pred_Y.shape)\n",
    "\n",
    "# Calculate precision, recall, and AUC for PR curve for each class\n",
    "for i in range(Y.shape[1]):\n",
    "    \n",
    "    y_pred = (pred_Y[:, i] > 0.5).astype(int)\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = precision_score(test_Y[:, i], y_pred)\n",
    "    recall = recall_score(test_Y[:, i], y_pred)\n",
    "    \n",
    "    print(token_order[i])\n",
    "    print(\n",
    "        precision,\n",
    "        recall\n",
    "    )\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AL loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Iteration 1 --- \n",
      "--- Epoch 1 ---\n",
      "previous shapes:  (70733, 1024) (70733, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new shapes:  (36015, 1024) (36015, 4)\n",
      "\n",
      "Epoch 1: saving model to /home/ec2-user/acoustic-AL/models/least_confidence_sampling/training/checkpoint.weights.h5\n",
      "1126/1126 - 4s - loss: 0.0320 - recall_2: 0.0460 - precision_2: 0.4000 - auc_pr: 0.1245 - metric/map: 0.0225 - 4s/epoch - 3ms/step\n",
      "--- Epoch 2 ---\n",
      "previous shapes:  (70733, 1024) (70733, 4)\n",
      "new shapes:  (36015, 1024) (36015, 4)\n",
      "\n",
      "Epoch 1: saving model to /home/ec2-user/acoustic-AL/models/least_confidence_sampling/training/checkpoint.weights.h5\n",
      "1126/1126 - 3s - loss: 0.0256 - recall_2: 0.1030 - precision_2: 0.7181 - auc_pr: 0.2901 - metric/map: 0.0247 - 3s/epoch - 3ms/step\n",
      "--- Epoch 3 ---\n",
      "previous shapes:  (70733, 1024) (70733, 4)\n",
      "new shapes:  (36015, 1024) (36015, 4)\n",
      "\n",
      "Epoch 1: saving model to /home/ec2-user/acoustic-AL/models/least_confidence_sampling/training/checkpoint.weights.h5\n",
      "1126/1126 - 3s - loss: 0.0243 - recall_2: 0.1462 - precision_2: 0.7170 - auc_pr: 0.3344 - metric/map: 0.0250 - 3s/epoch - 3ms/step\n",
      "--- Epoch 4 ---\n",
      "previous shapes:  (70733, 1024) (70733, 4)\n",
      "new shapes:  (36015, 1024) (36015, 4)\n",
      "\n",
      "Epoch 1: saving model to /home/ec2-user/acoustic-AL/models/least_confidence_sampling/training/checkpoint.weights.h5\n",
      "1126/1126 - 3s - loss: 0.0225 - recall_2: 0.1811 - precision_2: 0.7315 - auc_pr: 0.3887 - metric/map: 0.0258 - 3s/epoch - 3ms/step\n",
      "--- Epoch 5 ---\n",
      "previous shapes:  (70733, 1024) (70733, 4)\n",
      "new shapes:  (36015, 1024) (36015, 4)\n",
      "\n",
      "Epoch 1: saving model to /home/ec2-user/acoustic-AL/models/least_confidence_sampling/training/checkpoint.weights.h5\n",
      "1126/1126 - 3s - loss: 0.0217 - recall_2: 0.2002 - precision_2: 0.7273 - auc_pr: 0.4182 - metric/map: 0.0261 - 3s/epoch - 3ms/step\n",
      "--- Epoch 6 ---\n",
      "previous shapes:  (70733, 1024) (70733, 4)\n",
      "new shapes:  (36015, 1024) (36015, 4)\n",
      "\n",
      "Epoch 1: saving model to /home/ec2-user/acoustic-AL/models/least_confidence_sampling/training/checkpoint.weights.h5\n",
      "1126/1126 - 3s - loss: 0.0205 - recall_2: 0.2327 - precision_2: 0.7423 - auc_pr: 0.4666 - metric/map: 0.0262 - 3s/epoch - 3ms/step\n",
      "--- Epoch 7 ---\n",
      "previous shapes:  (70733, 1024) (70733, 4)\n",
      "new shapes:  (36015, 1024) (36015, 4)\n",
      "\n",
      "Epoch 1: saving model to /home/ec2-user/acoustic-AL/models/least_confidence_sampling/training/checkpoint.weights.h5\n",
      "1126/1126 - 3s - loss: 0.0195 - recall_2: 0.2548 - precision_2: 0.7437 - auc_pr: 0.4861 - metric/map: 0.0267 - 3s/epoch - 3ms/step\n",
      "--- Epoch 8 ---\n",
      "previous shapes:  (70733, 1024) (70733, 4)\n",
      "new shapes:  (36015, 1024) (36015, 4)\n",
      "\n",
      "Epoch 1: saving model to /home/ec2-user/acoustic-AL/models/least_confidence_sampling/training/checkpoint.weights.h5\n",
      "1126/1126 - 3s - loss: 0.0183 - recall_2: 0.3125 - precision_2: 0.7927 - auc_pr: 0.5399 - metric/map: 0.0266 - 3s/epoch - 3ms/step\n",
      "--- Epoch 9 ---\n",
      "previous shapes:  (70733, 1024) (70733, 4)\n",
      "new shapes:  (36015, 1024) (36015, 4)\n",
      "\n",
      "Epoch 1: saving model to /home/ec2-user/acoustic-AL/models/least_confidence_sampling/training/checkpoint.weights.h5\n",
      "1126/1126 - 3s - loss: 0.0175 - recall_2: 0.3221 - precision_2: 0.7731 - auc_pr: 0.5668 - metric/map: 0.0269 - 3s/epoch - 3ms/step\n",
      "--- Epoch 10 ---\n",
      "previous shapes:  (70733, 1024) (70733, 4)\n",
      "new shapes:  (36015, 1024) (36015, 4)\n",
      "\n",
      "Epoch 1: saving model to /home/ec2-user/acoustic-AL/models/least_confidence_sampling/training/checkpoint.weights.h5\n",
      "1126/1126 - 3s - loss: 0.0163 - recall_2: 0.3886 - precision_2: 0.7825 - auc_pr: 0.6076 - metric/map: 0.0272 - 3s/epoch - 3ms/step\n",
      "2763/2763 [==============================] - 3s 1ms/step\n",
      "  91/8842 [..............................] - ETA: 9s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 05:12:56.626090: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1158885376 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5456/8842 [=================>............] - ETA: 3s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m         classwize[i] \u001b[38;5;241m=\u001b[39m (precision, recall)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m classwize    \n\u001b[0;32m---> 24\u001b[0m model, iteration_metrics, histories \u001b[38;5;241m=\u001b[39m \u001b[43mAL_simulation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43munlabelled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43munlabelled_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlabelled_Y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintial_labelled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabelled_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabelled_Y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_Y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleast_confidence_sampling\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluate_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision_recall\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43munlabelled_X\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# iterations\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleast_confidence_sampling\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 40\u001b[0m, in \u001b[0;36mAL_simulation\u001b[0;34m(unlabelled, intial_labelled, test, model, name, sampling_query, pool_size, evaluate_metrics, num_iterations, epochs)\u001b[0m\n\u001b[1;32m     37\u001b[0m iteration_metrics[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabelling_budget=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbudget\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m evaluate_metrics(y_true, y_pred)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# active learning query on unlabelled set\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43munlabelled_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m indexes_pool \u001b[38;5;241m=\u001b[39m sampling_query(y_pred, unlabelled_X, pool_size)\n\u001b[1;32m     42\u001b[0m X_new \u001b[38;5;241m=\u001b[39m [unlabelled_X[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indexes_pool]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/src/engine/training.py:2655\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2653\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m   2654\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m-> 2655\u001b[0m     tmp_batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   2657\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:854\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the graph function.\"\"\"\u001b[39;00m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m--> 854\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_type_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanonicalize_function_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_is_pure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    861\u001b[0m args, kwds \u001b[38;5;241m=\u001b[39m bound_args\u001b[38;5;241m.\u001b[39margs, bound_args\u001b[38;5;241m.\u001b[39mkwargs\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    863\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    864\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py:422\u001b[0m, in \u001b[0;36mcanonicalize_function_inputs\u001b[0;34m(args, kwargs, function_type, default_values, is_pure)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_pure:\n\u001b[1;32m    421\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m _convert_variables_to_tensors(args, kwargs)\n\u001b[0;32m--> 422\u001b[0m bound_arguments \u001b[38;5;241m=\u001b[39m \u001b[43mbind_function_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_values\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bound_arguments\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py:442\u001b[0m, in \u001b[0;36mbind_function_inputs\u001b[0;34m(args, kwargs, function_type, default_values)\u001b[0m\n\u001b[1;32m    434\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    435\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName collision after sanitization. Please rename \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.function input parameters. Original: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msorted\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Sanitized: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msorted\u001b[39m(sanitized_kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m   )\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 442\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_defaults\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitized_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_values\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    447\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBinding inputs to tf.function failed due to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived args: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and kwargs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msanitized_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for signature:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m   ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/core/function/polymorphism/function_type.py:262\u001b[0m, in \u001b[0;36mFunctionType.bind_with_defaults\u001b[0;34m(self, args, kwargs, default_values)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Generate a proto representation from the FunctionType.\"\"\"\u001b[39;00m\n\u001b[1;32m    254\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m function_type_pb2\u001b[38;5;241m.\u001b[39mFunctionType(\n\u001b[1;32m    255\u001b[0m       parameters\u001b[38;5;241m=\u001b[39m[p\u001b[38;5;241m.\u001b[39mto_proto() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mvalues()],\n\u001b[1;32m    256\u001b[0m       captures\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    259\u001b[0m           \u001b[38;5;28;01mfor\u001b[39;00m n, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaptures\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    260\u001b[0m       ])\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_defaults\u001b[39m(\u001b[38;5;28mself\u001b[39m, args, kwargs, default_values):\n\u001b[1;32m    263\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns BoundArguments with default values filled in.\"\"\"\u001b[39;00m\n\u001b[1;32m    264\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def least_confidence_sampling(y_pred, unlabelled_X, pool_size):\n",
    "    # Calculate least confidence (1 - max probability)\n",
    "    least_confidence_scores = 1 - np.max(y_pred, axis=1)\n",
    "\n",
    "    # Select the least confident instances\n",
    "    uncertain_indices = np.argsort(least_confidence_scores)[:pool_size]\n",
    "    return uncertain_indices\n",
    "\n",
    "def precision_recall(y_true, y_pred):\n",
    "    classwize = {}\n",
    "    for i in range(4):\n",
    "        m = keras.metrics.Precision(thresholds=0.5)\n",
    "        m.update_state(y_true[:, i], y_pred[:, i])\n",
    "        precision = m.result().numpy()\n",
    "        \n",
    "        m = keras.metrics.Recall(thresholds=0.5)\n",
    "        m.update_state(y_true[:, i], y_pred[:, i])\n",
    "        recall = m.result().numpy()\n",
    "        \n",
    "        classwize[i] = (precision, recall)\n",
    "    \n",
    "    return classwize    \n",
    "\n",
    "model, iteration_metrics, histories = AL_simulation(\n",
    "    \n",
    "        unlabelled=(unlabelled_X, unlabelled_Y), \n",
    "        intial_labelled=(labelled_X, labelled_Y), \n",
    "        test=(test_X, test_Y),\n",
    "        model=model, \n",
    "        name='least_confidence_sampling', \n",
    "        evaluate_metrics=precision_recall,\n",
    "        pool_size= int(len(unlabelled_X) / 20), # iterations\n",
    "        num_iterations=20,\n",
    "        sampling_query=least_confidence_sampling\n",
    "        \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.4834437, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7053292, shape=(), dtype=float32)\n",
      "tf.Tensor(0.30769232, shape=(), dtype=float32)\n",
      "tf.Tensor(0.3271028, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "pred_Y = model(test_X)\n",
    "\n",
    "# look at all precisions and recalls\n",
    "for i in range(4):\n",
    "    m = keras.metrics.Precision(thresholds=0.5)\n",
    "    m.update_state(test_Y[:, i], pred_Y[:, i])\n",
    "    print(m.result())\n",
    "\n",
    "    m = keras.metrics.Recall(thresholds=0.5)\n",
    "    m.update_state(test_Y[:, i], pred_Y[:, i])\n",
    "    print(m.result())\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, batched_test_dataset):\n",
    "    y_true, y_pred = [], []\n",
    "    for batch_X, batch_y in batched_test_dataset, desc='loading/predicting test ds':\n",
    "        predictions = model(batch_X, training=False)\n",
    "        y_true.extend(batch_y.numpy())\n",
    "        y_pred.extend(predictions.numpy()) \n",
    "\n",
    "    return np.array(y_true), np.array(y_pred)\n",
    "\n",
    "\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    y_true_flat = y_true.reshape(-1, 4)\n",
    "    y_pred_flat = y_pred.reshape(-1, 4)\n",
    "    \n",
    "    m = keras.metrics.AUC(curve='roc')\n",
    "    m.update_state(y_true_flat[:, 0], y_pred_flat[:, 0])\n",
    "    m.result()\n",
    "    \n",
    "    m = keras.metrics.AUC(curve='pr')\n",
    "    m.update_state(y_true_flat[:, 0], y_pred_flat[:, 0])\n",
    "    m.result()\n",
    "    \n",
    "    for i in range(4):\n",
    "        print('\\n class ', i)\n",
    "        y_pred_binary = (y_pred_flat[:, i] >= threshold).astype(int)\n",
    "\n",
    "        precision_metric = Precision()\n",
    "        recall_metric = Recall()\n",
    "\n",
    "        precision_metric.update_state(y_true_flat[:, i], y_pred_binary)\n",
    "        recall_metric.update_state(y_true_flat[:, i], y_pred_binary)\n",
    "\n",
    "        precision = precision_metric.result().numpy()\n",
    "        recall = recall_metric.result().numpy()\n",
    "\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        \n",
    "        ...\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
